{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52799027-7c0b-4c79-bdda-b4b1d5c37f11",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d73f828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.io import arff\n",
    "\n",
    "from sklearn import ensemble, neighbors, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bc6c1-0d34-4b6c-95f2-53ea445eb086",
   "metadata": {},
   "source": [
    "# Step 1: Remove Collinear Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7a1c39a-6703-4575-aaab-1ce65f23f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for performing feature selection for machine learning or data preprocessing\n",
    "class feature_select():\n",
    "    def __init__(self, data, labels=None):\n",
    "        \n",
    "        # Dataset and optional training labels\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Dataframes recording information about features to remove\n",
    "        self.record_collinear = None\n",
    "\n",
    "        # Dictionary to hold removal operations\n",
    "        self.ops = {}\n",
    "        \n",
    "    # Finds collinear features based on the correlation coefficient between features.\n",
    "    def identify_collinear(self, correlation_threshold):\n",
    "        \n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        corr_matrix = self.data.corr()\n",
    "        \n",
    "        # Upper triangle of the correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(bool))\n",
    "        \n",
    "        # Select features with correlation value above the threshold, based on absolute values\n",
    "        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "        # Iterate through the columns to drop to record pairs of correlated features\n",
    "        for column in to_drop:\n",
    "\n",
    "            # Find the correlated features\n",
    "            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "            # Find the correlated values\n",
    "            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "            drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "            # Record the information (need a temp df for now)\n",
    "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                             'corr_feature': corr_features,\n",
    "                                             'corr_value': corr_values})\n",
    "            # Add to dataframe\n",
    "            record_collinear = pd.concat([record_collinear,temp_df], axis=0, ignore_index=True)\n",
    "            \n",
    "        self.record_collinear = record_collinear\n",
    "        self.ops['collinear'] = to_drop\n",
    "        \n",
    "        print('%d features with a correlation magnitude greater than %0.2f.\\n' % (len(self.ops['collinear']), self.correlation_threshold))\n",
    "    \n",
    "    # Check the identified features before removal. Returns a list of the unique features identified.\n",
    "    def check_removal(self):\n",
    "        self.all_identified = set(list(chain(*list(self.ops.values()))))\n",
    "        print('Total of %d features identified for removal' % len(self.all_identified))\n",
    "        return list(self.all_identified)\n",
    "        \n",
    "    # Remove the features from the data\n",
    "    def remove(self, methods):\n",
    "        features_to_drop = []\n",
    "        data = self.data\n",
    "            \n",
    "        # Iterate through the specified methods\n",
    "        for method in methods:\n",
    "            \n",
    "            # Check to make sure the method has been run\n",
    "            if method not in self.ops.keys():\n",
    "                raise NotImplementedError('%s method has not been run' % method)\n",
    "                \n",
    "            # Append the features identified for removal\n",
    "            else:\n",
    "                features_to_drop.append(self.ops[method])\n",
    "    \n",
    "        # Find the unique features to drop\n",
    "        features_to_drop = set(list(chain(*features_to_drop)))\n",
    "        features_to_drop = list(features_to_drop)\n",
    "        \n",
    "        # Remove the features and return the data\n",
    "        data = data.drop(columns = features_to_drop)\n",
    "        self.removed_features = features_to_drop\n",
    "        \n",
    "        print('Removed %d features.' % len(features_to_drop))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e5137-a0a0-45c0-96d3-e254da085d48",
   "metadata": {},
   "source": [
    "# Step 2: Select a dataset (in csv format) and run a classifier of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81c5a53c-e5fa-4371-a535-accbd044efd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features=30\n",
      "4 features with a correlation magnitude greater than 0.98.\n",
      "\n",
      "      drop_feature    corr_feature  corr_value\n",
      "0   perimeter_mean     radius_mean    0.997855\n",
      "1        area_mean     radius_mean    0.987357\n",
      "2        area_mean  perimeter_mean    0.986507\n",
      "3  perimeter_worst    radius_worst    0.993708\n",
      "4       area_worst    radius_worst    0.984015\n",
      "Total of 4 features identified for removal\n",
      "['perimeter_mean', 'area_mean', 'area_worst', 'perimeter_worst']\n",
      "Removed 4 features.\n",
      "Updated number of features=26\n",
      "rep= 0\n",
      "rep= 1\n",
      "rep= 2\n"
     ]
    }
   ],
   "source": [
    "#Number of times to repeat the experiment and report average AUC results\n",
    "repeat = 3\n",
    "\n",
    "#Select a classifier to check the convergence and sample size for the dataset\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "\n",
    "# Read Dataset\n",
    "df = pd.read_csv(\"Wisconsin_Preprocessed.csv\")\n",
    "y = df.iloc[:,-1]\n",
    "X = df.iloc[:,0:-1]\n",
    "\n",
    "X=X.astype(\"float\")\n",
    "num_features = len(X.T)\n",
    "print(\"Original number of features=\"+str(num_features))\n",
    "\n",
    "#Remove collinear features\n",
    "fs = FeatureSelector(data = X, labels = y)\n",
    "fs.identify_collinear(correlation_threshold=0.98)\n",
    "\n",
    "# information about collinear features selected for removal\n",
    "correlated_features = fs.ops['collinear']\n",
    "print(fs.record_collinear)\n",
    "all_to_remove = fs.check_removal()\n",
    "print(all_to_remove)\n",
    "\n",
    "train_removed_all_once = fs.remove(methods = ['collinear'])\n",
    "X = train_removed_all_once\n",
    "\n",
    "num_features = len(X.T)\n",
    "print(\"Updated number of features=\"+str(num_features))\n",
    "\n",
    "# Maximum training samples upto which the experiment is run is either 80% of the data or 50000, whichever is less\n",
    "max_samples = int(0.8*len(y))\n",
    "\n",
    "if max_samples >50000:\n",
    "    max_samples = 50000\n",
    "    \n",
    "#Randomly generated list of samples to be picked for the study    \n",
    "s = list(range(max_samples))\n",
    "random.shuffle(s)\n",
    "\n",
    "auc_list = []\n",
    "samplesize_list = []\n",
    "\n",
    "for rep in range(repeat):  \n",
    "    print(\"rep=\",rep)\n",
    "    #randomize the samples to be picked. Samples build up on earlier samples for each run.\n",
    "    random.shuffle(s)\n",
    "    \n",
    "    #Resplitting the samples till class imbalance is resolved\n",
    "    num_attempts=0\n",
    "    class_imbalance=1\n",
    "    while class_imbalance == 1:\n",
    "        num_attempts+=1\n",
    "        if num_attempts>10:\n",
    "            break\n",
    "        #test train split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, shuffle=True)\n",
    "        c = Counter(y_train)\n",
    "        v1 = c.values()\n",
    "        if len(c)==1:\n",
    "            class_imbalance=1\n",
    "        else:\n",
    "            imb=\"\"\n",
    "            for v in c.values():\n",
    "                if v<=5: # Resampling if training samples in each class is <=5 (when sample size is small this problem might come up)\n",
    "                    imb+=\"1\"\n",
    "                else:\n",
    "                    imb+=\"0\"\n",
    "            if imb==\"00\":\n",
    "                class_imbalance = 0\n",
    "                break\n",
    "            else:\n",
    "                class_imbalance = 1\n",
    "                \n",
    "    if num_attempts>10:\n",
    "        print(\"attempts over\")\n",
    "        continue\n",
    "        \n",
    "    #Normalize the training data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    scaler.transform(X_train)\n",
    "    #Normalize the test data based on training data parameters\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    #Start with sample size 50 generate random indices to create samples\n",
    "    sample_size = 50\n",
    "    auc = []\n",
    "    \n",
    "    while sample_size <= max_samples:\n",
    "        samplesize_list.append(sample_size)\n",
    "        train_sample = s[-sample_size:]\n",
    "        X_exp = X_train.iloc[train_sample]\n",
    "        y_exp = y_train.iloc[train_sample]\n",
    "        \n",
    "        #chck for less than 5 samples per class\n",
    "        class_imbalance = 1\n",
    "        c = Counter(y_exp)\n",
    "        if len(c)==1:\n",
    "            class_imbalance=1\n",
    "        else:\n",
    "            imb=\"\"\n",
    "            for v in c.values():\n",
    "                if v<=5: # if training samples in each class is <=5 (for better class representation)\n",
    "                    imb+=\"1\"\n",
    "                else:\n",
    "                    imb+=\"0\"\n",
    "            if imb==\"00\":\n",
    "                class_imbalance = 0\n",
    "                \n",
    "            else:\n",
    "                class_imbalance = 1\n",
    "        if class_imbalance == 1:\n",
    "            auc.append(0)\n",
    "\n",
    "        else:            \n",
    "            model = clf.fit(X_exp, y_exp)\n",
    "            #model = GridSearchCV(clf[i], params[i], cv=5, scoring='accuracy') # You can also perform a grid search with crossvalidation
    "        \n",
    "            y_pred = clf.predict(X_test)\n",
    "            auc.append(round(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]),4))\n",
    "\n",
    "\n",
    "        #Sample size in the experiment is incremented in steps of 10 till 200, then in steps of 100 upto 1000 and finally in steps of 500 after that.\n",
    "        if sample_size < 200 and sample_size < max_samples:\n",
    "            sample_size_incr = 10\n",
    "        elif sample_size >= 200 and sample_size < 1000 and sample_size < max_samples:\n",
    "            sample_size_incr =    100\n",
    "        elif sample_size >= 1000 and sample_size < max_samples:\n",
    "            sample_size_incr = 500\n",
    "\n",
    "        sample_size = sample_size + sample_size_incr\n",
    "    auc_list.append(auc)\n",
    "    \n",
    "#find average metrics\n",
    "rep_auc = [sum(e)/len(e) for e in zip(*auc_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bdb4794-cdee-4d72-a93d-2c52e86facbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC at different sample sizes: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.921267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>0.929733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.932833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>0.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>0.942767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>0.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>110</td>\n",
       "      <td>0.947867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>120</td>\n",
       "      <td>0.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>130</td>\n",
       "      <td>0.941367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>140</td>\n",
       "      <td>0.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>0.949333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>160</td>\n",
       "      <td>0.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>170</td>\n",
       "      <td>0.953833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>180</td>\n",
       "      <td>0.953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>190</td>\n",
       "      <td>0.953367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>0.953633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>300</td>\n",
       "      <td>0.955067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>400</td>\n",
       "      <td>0.954900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1\n",
       "0    50  0.921267\n",
       "1    60  0.929733\n",
       "2    70  0.932833\n",
       "3    80  0.937200\n",
       "4    90  0.942767\n",
       "5   100  0.945800\n",
       "6   110  0.947867\n",
       "7   120  0.938400\n",
       "8   130  0.941367\n",
       "9   140  0.947100\n",
       "10  150  0.949333\n",
       "11  160  0.953000\n",
       "12  170  0.953833\n",
       "13  180  0.953800\n",
       "14  190  0.953367\n",
       "15  200  0.953633\n",
       "16  300  0.955067\n",
       "17  400  0.954900"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ss_auc = pd.DataFrame(zip(samplesize_list,rep_auc))\n",
    "print(\"AUC at different sample sizes: \")\n",
    "df_ss_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663a336-009e-48b3-90b8-dcc3f6b5f50f",
   "metadata": {},
   "source": [
    "# Step 3: Checking for AUC convergence and finding the minimum sample size required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d713f962-8f71-4dba-9e63-eb03afc26eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sample Size =  569\n",
      "Convergence Sample Size =  100\n",
      "Maximum AUC =  0.9549\n",
      "Convergence AUC =  0.9458\n"
     ]
    }
   ],
   "source": [
    "#Convert the AUC scores for incremental sample size into a dataframe for ease of calculations\n",
    "df = pd.DataFrame(rep_auc)\n",
    "\n",
    "#Set hyperparameters\n",
    "error_threshold=0.01\n",
    "\n",
    "total_sample = len(y)\n",
    "sample_list = [50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,300,400,500,600,700,800,900,1000]\n",
    "\n",
    "istart=0\n",
    "iend = istart + 1\n",
    "flag=0\n",
    "final_auc = round(float(df.iloc[-1]),4)\n",
    "if final_auc > 0.6:\n",
    "    v = len(df)\n",
    "    for j in range(v):\n",
    "        j=j+2\n",
    "        auc_next = round(float(df.iloc[j]),4)\n",
    "        if(auc_next>=final_auc-error_threshold):\n",
    "            iend=j\n",
    "            if iend<=23:\n",
    "                css=sample_list[iend]\n",
    "            else:\n",
    "                iend1=iend-23\n",
    "                css=1000+iend1*500\n",
    "            convergence_auc = auc_next    \n",
    "            flag=1\n",
    "            break\n",
    "        \n",
    "    \n",
    "    if flag==0:\n",
    "        print(\"conv index=\",-1)\n",
    "\n",
    "else:\n",
    "    print(\"conv index=\",-1)\n",
    "\n",
    "print(\"Total Sample Size = \", total_sample)\n",
    "print(\"Convergence Sample Size = \", css)\n",
    "print(\"Maximum AUC = \", final_auc)\n",
    "print(\"Convergence AUC = \", convergence_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f8317-d96f-423f-be38-7378d32d95a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98b894-ddcb-4341-adc0-af32bc1184e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
